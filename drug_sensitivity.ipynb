{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RYhQFvsLkn7"
      },
      "source": [
        "# DTSA-5511 Final Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90LIiyspLoyY"
      },
      "source": [
        "For this project, I decided I wanted to use this as an opportunity to learn about bioinformatics and pharmacology. I started by searching for a topic and dataset to base the project on, and I came across the Genomics of Drug Sensitivity in Cancer ([GDSC](https://www.cancerrxgene.org/)) data repository. GDSC has collected data on hundreds of different drugs with measurements of their efficacy in inhibiting the growth of cancer cells (the GDSC data contains drug inihibition measurements for approximately 1000 cancer cell lines).\n",
        "\n",
        "Looking through the GDSC website, I soon realized that it would be too difficult for a non-expert like me to assemble a dataset for this project from \"scratch\". Fortunately, I found the [PyTDC](https://pypi.org/project/pytdc/) project (from the [Therapeutics Data Commons](https://tdcommons.ai/) group), which offers a packaged version of the GDSC raw data in a ready-to-use form for ML. Then all I had to do to access the GDSC data was to install the `PyTDC` library and import the dataframe into Pandas.\n",
        "\n",
        "With the GDSC data in hand, as I started doing some EDA and transformations, I quickly found that my local machine, even with 32 GB of RAM, was not going to be up to the task of manipulating dataframes on the order of ~175,000 rows and ~20,000 columns. I thought about condensing the dataset into a randomly sampled subset, but I was concerned that this would hinder model performance. In the end, I decided to use a Google Colab environment with 171 GB RAM and an NVIDIA A100 GPU. This came with its own set of difficulties, but utimately Colab was able to support the compute needs for the project. This turned out to be a good lesson in the pitfalls of managing a \"large\" dataset.\n",
        "\n",
        "### **Spoiler!**\n",
        "The final results from the models are not very strong. I need to consult with domain experts and do more research to understand how to improve these."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxaAFRJKLu5e"
      },
      "source": [
        "## Project Goal and Background Info\n",
        "\n",
        "The goal of this project is to build a deep learning model that predicts a drug's efficacy at inhibiting the growth of cancer cells. We'll start by describing the data in the GDSC dataset to understand how the model can make its predictions.\n",
        "\n",
        "The dataset is structured around drug/cell line pairings. Each row represents an experiment where a specific drug was tested on a particular cancer cell line*.\n",
        "\n",
        "### Model Input\n",
        "The model will use two main types of input data to make its predictions.\n",
        "\n",
        "1) **Cell Line:** A large portion of each row in the dataset is dedicated to a cancer cell's **gene expression profile**. This profile is represented by approximately 17,000 columns, where each column corresponds to a specific gene. The value in each of these columns quantifies that gene's activity level—indicating whether it is more or less active (\"turned on\" or \"off\") in that particular cancer cell. Together, these columns provide a detailed molecular snapshot of the cancer cell for the model to learn from.\n",
        "\n",
        "2) **Drug:** For each row in the data, there is also sizeable number of columns dedicated to the **drug's molecular structure**. The molecular structure is first encoded in what is called a [SMILES (Simplified Molecular-Input Line-Entry System)](https://en.wikipedia.org/wiki/Simplified_Molecular_Input_Line_Entry_System) string. For inputing the drug's molecular data into the neural network, we convert the SMILES string into numerical form using a method called a [Morgan Fingerprint](https://darkomedin-datascience.medium.com/data-science-for-drug-discovery-research-morgan-fingerprints-using-alanine-and-testosterone-92a2c69dd765). This process transforms the chemical structure of each drug into a fixed-length vector of 0s and 1s (2048 bits long). Each position in the vector represents the presence or absence of specific chemical components. These 2048 fingerprint values become columns in the dataset, providing the model with a detailed numerical profile of the drug's chemical features.\n",
        "\n",
        "### Model Output\n",
        "\n",
        "The model's target variable is the log of the measured **IC50 value** for each row's drug/cell line pairing. [IC50](https://en.wikipedia.org/wiki/IC50) is a standard measure of a drug's potency. It represents the concentration of a drug required to inhibit a biological process (such as cancer cell growth) by 50%. To determine this value, cell samples are exposed in a laboratory to varying concentrations of the drug. After an incubation period, [cell viability](https://en.wikipedia.org/wiki/Viability_assay) is measured (often by measuring light intensity shone through the samples, which correlates with the number of living cells). A dose-response curve, typically a [4-parameter logistic curve](https://www.quantics.co.uk/blog/what-is-the-4pl-formula/), is then fitted to these measurements to calculate the IC50 value.\n",
        "\n",
        "Therefore, the objective is to train a model that can take a cell line's gene expression profile and a drug's chemical structure as input and accurately predict the resulting IC50 value. This is a regression task in the output of the neural network.\n",
        "\n",
        "*A cell line is a population of cancer cells that have been sampled from a patient and re-grown in a laboratory. Different cell lines can represent one type of cancer (for example, breast cancer), yet each cell line is genetically unique as it originates from one particular patient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j9g_M-ZL7CU"
      },
      "source": [
        "## Data Preparation and EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9U4O0yVqaYc"
      },
      "outputs": [],
      "source": [
        "!pip install PyTDC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24281d3a"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LheOdO8hpmX6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from tdc.multi_pred import DrugRes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKTSKQA4P0iM"
      },
      "source": [
        "### First look at the data\n",
        "\n",
        "We'll use the [DrugRes](https://tdcommons.ai/multi_pred_tasks/drugres/) object to create a Pandas dataframe containing the GDSC data.\n",
        "\n",
        "The dataframe initially has 5 columns:\n",
        "\n",
        "`Drug_ID`: Commerical drug name.  \n",
        "`Drug`: Column containing the SMILES string that encodes the molecular structure for the drug.  \n",
        "`Cell Line_ID`: GDSC [cell line ID](https://www.cancerrxgene.org/celllines).  \n",
        "`Cell Line`: Column where each row stores an array of 17737 gene expressions for the cancer cell line.  \n",
        "`Y`: Measured IC50 value for the drug/cell line pair.  \n",
        "\n",
        "From the first 5 rows, we can see that one drug is paired with multiple cell lines, with a different IC50 measurement for each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "XXhZOFfiptOl",
        "outputId": "a621d3b1-c70e-45d4-ea0c-5745993e6caa"
      },
      "outputs": [],
      "source": [
        "# Load the GDSC dataset from TDC\n",
        "data = DrugRes(name = 'gdsc1')\n",
        "\n",
        "# Get the entire dataset as a pandas dataframe\n",
        "gdsc1_df = data.get_data()\n",
        "\n",
        "gdsc1_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7UvleEJP6Ms"
      },
      "source": [
        "### Gene Labels Transformation\n",
        "\n",
        "Rather than having a single 17737-long gene expression array in each row, it would be better for analysis to put each gene in its own labeled column. To do this, first we need to get the gene labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2Mx12MgpzPq",
        "outputId": "561e1007-2b52-4f7d-ea8d-49321527a50d"
      },
      "outputs": [],
      "source": [
        "# Get the gene symbols\n",
        "gene_labels = data.get_gene_symbols()\n",
        "print(f\"Total number of genes: {len(gene_labels)}\")\n",
        "print(f\"\\nFirst 20 gene symbols:\")\n",
        "print(gene_labels[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJI3DsdNp1gO",
        "outputId": "60849c87-da2f-48f1-d830-61e61589e6f8"
      },
      "outputs": [],
      "source": [
        "type(gene_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFN36nZcQAO3"
      },
      "source": [
        "Unpack the long `Cell Line` arrays and stack them so that the elements fall into labeled columns. This creates `gene_expression_df` with 177310 rows and 17737 columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xy_rP_Q4z4jG"
      },
      "outputs": [],
      "source": [
        "gene_expression_df = pd.DataFrame(\n",
        "    np.stack(gdsc1_df['Cell Line'].values),\n",
        "    columns=gene_labels,\n",
        "    dtype=np.float32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "h6XclIRsWXCA",
        "outputId": "4251c0f3-5aaa-49ce-9153-bf0bad597a27"
      },
      "outputs": [],
      "source": [
        "gene_expression_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woe0QKuMGKc6",
        "outputId": "0ef94fda-6d9c-40e5-ff65-f3ed7cd39807"
      },
      "outputs": [],
      "source": [
        "gene_expression_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQRD4wb8QFub"
      },
      "source": [
        "Reattach the `gene_expression_df` with the `Drug_ID`, `Drug`, and `Cell Line_ID` columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "n6DL7015mPa2",
        "outputId": "b63b9b1a-bef5-4653-92f4-4886ed49b44b"
      },
      "outputs": [],
      "source": [
        "# Extract non-genomics columns from the original gdsc1_df dataframe\n",
        "gdsc1_meta_df = gdsc1_df[['Drug_ID', 'Drug', 'Cell Line_ID', 'Y']]\n",
        "\n",
        "# Concatenate with the new gene_expression_df\n",
        "gdsc1_processed_df = pd.concat([gdsc1_meta_df, gene_expression_df], axis=1)\n",
        "\n",
        "print(f\"Shape of the gdsc1_processed_df dataframe: {gdsc1_processed_df.shape}\")\n",
        "gdsc1_processed_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJFrUdFyQpgi"
      },
      "source": [
        "### Check for missing values\n",
        "\n",
        "No null or missing values in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8cnPj8Owe-a",
        "outputId": "1895ece4-9bcd-4004-afd3-6cc84d93fdea"
      },
      "outputs": [],
      "source": [
        "gdsc1_processed_df.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0wpVYrMQK2Y"
      },
      "source": [
        "### IC50 Analysis\n",
        "\n",
        "The summary statistics for the `Y` column show a fairly wide range of values. We see negative numbers because the values are not the drug concentration itself, but rather the natural log of the concentration of the drug in µM.\n",
        "\n",
        "These values represent how effective a drug is at inhibiting cancer cell growth. The fact that the mean is less than the median (the 50th percentile) indicates that the distribution of IC50 values is skewed to the left. This means there is a tail of potent drug-cell interactions (very low IC50 values) that pull the overall average down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "6ZZ_Cx69n3WF",
        "outputId": "c4c892a7-26a9-4b46-873e-b65d79006743"
      },
      "outputs": [],
      "source": [
        "gdsc1_processed_df['Y'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DduHPrDBQPQ3"
      },
      "source": [
        "In the plot, the slight skew to the left confirms the prevalence of lower IC50 values. Despite the skew, the plot qualitatively looks \"normal\" enough with no obvious outliers, so we can proceed with this for regression modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "c4f32aa8",
        "outputId": "c718da8a-019b-4ff8-f978-f1ef999f5128"
      },
      "outputs": [],
      "source": [
        "# Histogram of the 'Y' variable\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(gdsc1_processed_df['Y'], kde=True)\n",
        "plt.title('Distribution of log(IC50) Values (Y)')\n",
        "plt.xlabel('log(IC50) Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7UwWNGsQUd6"
      },
      "source": [
        "### Drug Analysis\n",
        "\n",
        "The dataset contains **208 unique drugs**. The frequency of each drug varies widely; the most common drug is tested in **1,837** experiments, while the least common is tested in only **372**. This imbalance is visible in the distribution plot, where a few drugs account for a large portion of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyaAw-9ao8T4",
        "outputId": "e9836ca7-5cc5-4e30-a220-db2c6add21b5"
      },
      "outputs": [],
      "source": [
        "gdsc1_processed_df['Drug_ID'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOruz6zQrC59",
        "outputId": "34ec9b3c-a2f6-471a-eb0d-9d5395e92a2e"
      },
      "outputs": [],
      "source": [
        "gdsc1_processed_df['Drug_ID'].value_counts().max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92Gdys_JrGTt",
        "outputId": "0229ad37-7829-44f9-b9b4-f54ea81013fe"
      },
      "outputs": [],
      "source": [
        "gdsc1_processed_df['Drug_ID'].value_counts().min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "Aw4aTADkqjjb",
        "outputId": "4a81f032-00ab-48e7-8e1e-cf2eb6549c23"
      },
      "outputs": [],
      "source": [
        "gdsc1_processed_df['Drug_ID'].value_counts().head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "I2zNPQlQrksD",
        "outputId": "11ee3bb9-c92d-467f-a51f-1e26cf9e72e4"
      },
      "outputs": [],
      "source": [
        "drug_ID_counts = gdsc1_processed_df['Drug_ID'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "drug_ID_counts.plot(kind=\"barh\", color='lightblue')\n",
        "plt.xlabel(\"Count\", fontsize=8)\n",
        "plt.ylabel(\"Drug ID\", fontsize=8)\n",
        "plt.title(\"Drug ID Distribution\", fontsize=10)\n",
        "plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n",
        "plt.tick_params(axis='y', labelleft=False) # Hide y-axis labels\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihEV0tMcQbuz"
      },
      "source": [
        "### Cell Line Analysis\n",
        "\n",
        "There are **958 unique cell lines** in the dataset. Similar to the drugs, the cell lines are also imbalanced. The most frequently used cell line appears in **221** experiments, whereas the least common appears in just **1** experiment. Looking at the distribution plot, overall this imbalance is less pronounced than with the drugs but still notable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjNlgMFBpA_E",
        "outputId": "32bce00a-8e13-47b5-ca1c-d9cefd87c516"
      },
      "outputs": [],
      "source": [
        "gdsc1_processed_df['Cell Line_ID'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVEZ7IJJrNSB",
        "outputId": "d97b5627-3441-4d88-fa26-67688118a8ab"
      },
      "outputs": [],
      "source": [
        "gdsc1_processed_df['Cell Line_ID'].value_counts().max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcydJOKMrVD8",
        "outputId": "c6bacedc-c991-4516-938f-eb8d4a9ebab5"
      },
      "outputs": [],
      "source": [
        "gdsc1_processed_df['Cell Line_ID'].value_counts().min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6OHCZDN5q6Ix",
        "outputId": "ab675e27-b4d3-4b65-cdd4-7ac5a40f6b9c"
      },
      "outputs": [],
      "source": [
        "gdsc1_processed_df['Cell Line_ID'].value_counts().head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "pTFHBqBpuSnR",
        "outputId": "b495a251-8caa-48ea-ffdc-20c210735a5d"
      },
      "outputs": [],
      "source": [
        "cell_line_counts = gdsc1_processed_df['Cell Line_ID'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "cell_line_counts.plot(kind=\"barh\", color='orange')\n",
        "plt.xlabel(\"Count\", fontsize=8)\n",
        "plt.ylabel(\"Cell Line ID\", fontsize=8)\n",
        "plt.title(\"Cell Line ID Distribution\", fontsize=10)\n",
        "plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n",
        "plt.tick_params(axis='y', labelleft=False) # Hide y-axis labels\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn86wQqzWR-Z"
      },
      "source": [
        "### Gene Expression Analysis\n",
        "\n",
        "The initial dataset contains 17,737 gene expression features. To begin analyzing them, we calculate the variance of each gene across all experiments. The plot below visualizes this variance, sorted from lowest to highest. It shows that a large majority of genes have a variance at or near zero, meaning their expression levels do not change much across different cell lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cdb068b",
        "outputId": "04bd3f14-be03-42a4-84f3-a926507300f0"
      },
      "outputs": [],
      "source": [
        "# Calculate the variance of each gene expression column\n",
        "gene_expression_variance = gene_expression_df.var()\n",
        "\n",
        "print(\"Variance of gene expression for first five genes:\")\n",
        "print(gene_expression_variance.head())\n",
        "\n",
        "# Sort the variances to see which genes have the highest/lowest variance\n",
        "print(\"\\nTop 10 genes with highest variance:\")\n",
        "print(gene_expression_variance.sort_values(ascending=False).head(10))\n",
        "\n",
        "print(\"\\nTop 10 genes with lowest variance:\")\n",
        "print(gene_expression_variance.sort_values(ascending=True).head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "QQ1dbs9Sx-IX",
        "outputId": "e3ea1ae4-53f3-4c00-f410-c95029551777"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "gene_expression_variance.sort_values().plot(kind=\"bar\", color='blue')\n",
        "plt.xlabel(\"Gene\", fontsize=10)\n",
        "plt.ylabel(\"Variance\", fontsize=10)\n",
        "plt.title(\"Gene Expression Variance Distribution\", fontsize=12)\n",
        "plt.tick_params(axis='x', labelbottom=False) # Hide x-axis labels (gene names)\n",
        "plt.tick_params(axis='y', labelsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjnnbYqTWk19"
      },
      "source": [
        "Many of these genes show little variance across the cell lines, which means that their predictive power is limited. We can do dimensionality reduction by removing a portion of the low-variance genes. To reduce dimensionality, we apply a variance threshold of 0.1 using the [VarianceThreshold](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html) method from scikit-learn. This filtering step retains 13,391 of the genes for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "0c5349d9",
        "outputId": "b311e85f-7922-40fc-a0f2-9877552c25d9"
      },
      "outputs": [],
      "source": [
        "# Define a variance threshold\n",
        "threshold = 0.1\n",
        "\n",
        "selector = VarianceThreshold(threshold=threshold)\n",
        "\n",
        "# Convert column names to strings\n",
        "gene_expression_df.columns = gene_expression_df.columns.astype(str)\n",
        "\n",
        "# Fit and transform the gene expression data\n",
        "gene_expression_thresholded = selector.fit_transform(gene_expression_df)\n",
        "\n",
        "# Get the names of the selected genes\n",
        "selected_genes = gene_expression_df.columns[selector.get_support()]\n",
        "\n",
        "# Create a new dataframe with the selected genes\n",
        "gene_expression_df_filtered = pd.DataFrame(gene_expression_thresholded, columns=selected_genes, index=gene_expression_df.index)\n",
        "\n",
        "print(f\"Original number of genes: {gene_expression_df.shape[1]}\")\n",
        "print(f\"Number of genes after variance thresholding (threshold={threshold}): {gene_expression_df_filtered.shape[1]}\")\n",
        "\n",
        "gene_expression_df_filtered.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peUEGsTbWp-K"
      },
      "source": [
        "### Drug Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "e50f43c7",
        "outputId": "ac60a090-07bd-4c1c-8e09-cf5e43e617fb"
      },
      "outputs": [],
      "source": [
        "# Retrieve unique drug IDs and their SMILES strings\n",
        "drug_smiles_df = gdsc1_processed_df[['Drug_ID', 'Drug']].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nNumber of unique drugs: {drug_smiles_df.shape[0]}\")\n",
        "\n",
        "drug_smiles_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2WYyUVWWx7y"
      },
      "source": [
        "The `Drug` column in the `drug_smiles_df` dataframe contains the SMILES strings for each unique drug. The SMILES string is not a suitable format for direct input into a neural network, so we need to convert these into the numerical **Morgan Fingerprint** representation using the [AllChem](https://www.rdkit.org/docs/source/rdkit.Chem.AllChem.html) package. This \"fingerprint\" allows the model to learn relationships between a drug's chemical features and its efficacy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "47cdda06",
        "outputId": "be25d767-7303-4574-ff64-063d975b3abc"
      },
      "outputs": [],
      "source": [
        "# Function to generate chemical fingerprints\n",
        "def generate_fingerprint(smiles, radius=2, nbits=2048):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is not None:\n",
        "        return np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nbits))\n",
        "    return np.array([0] * nbits) # Return a zero vector for invalid SMILES\n",
        "\n",
        "# Apply the function to the drug_smiles_df\n",
        "fingerprint_matrix = np.vstack(drug_smiles_df['Drug'].apply(generate_fingerprint))\n",
        "\n",
        "# Create a a new dataframe from the fingerprints_matrix\n",
        "fingerprint_df = pd.DataFrame(fingerprint_matrix, columns=[f'FP_{i}' for i in range(fingerprint_matrix.shape[1])])\n",
        "\n",
        "# Add the Drug_ID back to the fingerprint_df for merging\n",
        "fingerprint_df['Drug_ID'] = drug_smiles_df['Drug_ID']\n",
        "\n",
        "print(f\"\\nShape of the fingerprint DataFrame: {fingerprint_df.shape}\")\n",
        "\n",
        "fingerprint_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX_iOUQOW7ie"
      },
      "source": [
        "### Training and Test Set Preparation\n",
        "\n",
        "With the gene expression and drug fingerprint features prepared, the next step is to combine them into a single, unified dataframe. We merge the filtered gene expression data with the drug fingerprints based on their respective indices and `Drug_ID`. This `gdsc1_merged_df` will serve as the complete dataset for training and evaluating our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "5b88e96c",
        "outputId": "9d2e15e6-cf18-45d7-ccf8-534e5ca1726c"
      },
      "outputs": [],
      "source": [
        "# Merge gene_expression_df_filtered with fingerprint_df based on Drug_ID\n",
        "gdsc1_merged_df = pd.merge(\n",
        "    gdsc1_processed_df[['Drug_ID', 'Cell Line_ID', 'Y']], # Include metadata needed for split\n",
        "    gene_expression_df_filtered,\n",
        "    left_index=True,\n",
        "    right_index=True\n",
        ")\n",
        "\n",
        "gdsc1_merged_df = pd.merge(\n",
        "    gdsc1_merged_df,\n",
        "    fingerprint_df,\n",
        "    on='Drug_ID',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "print(f\"\\nShape of the gdsc1_merged_df: {gdsc1_merged_df.shape}\")\n",
        "\n",
        "gdsc1_merged_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_RzFy5JXDNA"
      },
      "source": [
        "To ensure that the model generalizes to new, unseen drugs, we must prevent data leakage between the training and test sets. A standard random split would be inappropriate here, as it could place data from the same drug into both sets. This would allow the model to \"memorize\" a drug's behavior rather than learn its properties.\n",
        "\n",
        "To address this, we use `GroupShuffleSplit` from scikit-learn, with `Drug_ID` as the grouping variable. This method guarantees that all data for any given drug is confined to either the training set or the test set, but not both. By doing this, we can more reliably assess the model's ability to predict the efficacy of drugs it has never seen before. We allocate 80% of the drugs for training and reserve the remaining 20% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJTXPiSSMvVZ",
        "outputId": "151fe54b-8d51-4ad1-8209-e3522354155e"
      },
      "outputs": [],
      "source": [
        "# Define the features (X) and the target (y)\n",
        "\n",
        "# Define X from the merged dataframe, excluding metadata and target\n",
        "X = gdsc1_merged_df.drop(columns=['Drug_ID', 'Cell Line_ID', 'Y'])\n",
        "y = gdsc1_merged_df['Y']\n",
        "groups = gdsc1_merged_df['Drug_ID']\n",
        "\n",
        "# Initialize the GroupShuffleSplit for train/test split\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "\n",
        "# The .split() method returns the indices for the train and test sets\n",
        "train_idx, test_idx = next(gss.split(X, y, groups))\n",
        "\n",
        "# Create the train and test sets using the indices\n",
        "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "train_drugs = set(gdsc1_merged_df.iloc[train_idx]['Drug_ID'])\n",
        "test_drugs = set(gdsc1_merged_df.iloc[test_idx]['Drug_ID'])\n",
        "common_drugs = train_drugs.intersection(test_drugs)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "print(f\"\\nNumber of drugs in training set: {len(train_drugs)}\")\n",
        "print(f\"Number of drugs in testing set: {len(test_drugs)}\")\n",
        "print(f\"Number of drugs common to both sets: {len(common_drugs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYLb-aKoXVSv"
      },
      "source": [
        "Finally, we separate the features into their respective inputs for the multi-input neural network. The gene expression data (`X_train_genes`, `X_test_genes`) and the drug fingerprint data (`X_train_fp`, `X_test_fp`) are prepared as two distinct numpy arrays. This structure will allow our model to process each type of information through its own specialized branch before merging them for the final prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4KhlosYw-SE",
        "outputId": "6b477172-d4b7-4af2-d519-d7c77c8119fb"
      },
      "outputs": [],
      "source": [
        "# Select the gene expression data for train and test sets directly from the filtered dataframe\n",
        "X_train_genes = gene_expression_df_filtered.iloc[train_idx].values\n",
        "X_test_genes = gene_expression_df_filtered.iloc[test_idx].values\n",
        "\n",
        "# Select the fingerprint data for train and test sets\n",
        "X_train_fp = X_train[fp_cols].values\n",
        "X_test_fp = X_test[fp_cols].values\n",
        "\n",
        "print(f\"X_train_genes shape: {X_train_genes.shape}\")\n",
        "print(f\"X_train_fp shape: {X_train_fp.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"\\nX_test_genes shape: {X_test_genes.shape}\")\n",
        "print(f\"X_test_fp shape: {X_test_fp.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd5e873d",
        "outputId": "f658c446-653b-4aef-e1da-2565ed6ca331"
      },
      "outputs": [],
      "source": [
        "print(\"\\nFirst 5 rows of X_train_genes:\")\n",
        "print(X_train_genes[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WONbW1uDXeWE"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "### Model 1\n",
        "\n",
        "Our first model is a multi-input neural network to process the two types of features simultaneously (gene expression and drug fingerprint data).\n",
        "\n",
        "**Input Branches**:  \n",
        "The model has two separate input branches:  \n",
        "1) **Gene Expression Branch**: This branch processes the 13,391 gene expression features. It consists of two dense layers (`Dense`) with ReLU activation, between which is a dropout layer (`Dropout`) to prevent overfitting.  \n",
        "2) **Drug Fingerprint Branch**: This branch handles the 2048 drug fingerprint features, also using a sequence of dense and dropout layers.\n",
        "\n",
        "**Merging**: The outputs from the two branches are then concatenated into a single vector.\n",
        "\n",
        "**Fully Connected Layers**: This merged vector is passed through two more dense layers, with dropout applied after the first, to allow the model to learn from the combined features.\n",
        "\n",
        "**Output Layer**: The final layer is a single densely-connected neuron with a linear activation function, which outputs the predicted log(IC50) value, making it a regression model.\n",
        "\n",
        "The model is compiled using the Adam optimizer and the mean squared error (MSE) loss function, which is appropriate for this regression task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "DfzllhJZw5Xr",
        "outputId": "d0e4996d-e6e1-401a-d2d3-a9142c92bca6"
      },
      "outputs": [],
      "source": [
        "# Model 1\n",
        "\n",
        "# Gene Expression Branch\n",
        "gene_input = Input(shape=(X_train_genes.shape[1],), name='gene_input')\n",
        "gene_branch = Dense(256, activation='relu')(gene_input)\n",
        "gene_branch = Dropout(0.3)(gene_branch)\n",
        "gene_branch = Dense(128, activation='relu')(gene_branch)\n",
        "\n",
        "# Drug Fingerprint Branch\n",
        "fp_input = Input(shape=(X_train_fp.shape[1],), name='fp_input')\n",
        "fp_branch = Dense(128, activation='relu')(fp_input)\n",
        "fp_branch = Dropout(0.3)(fp_branch)\n",
        "fp_branch = Dense(64, activation='relu')(fp_branch)\n",
        "\n",
        "# Merge Branches\n",
        "merged = Concatenate()([gene_branch, fp_branch])\n",
        "merged = Dense(256, activation='relu')(merged)\n",
        "merged = Dropout(0.4)(merged)\n",
        "merged = Dense(128, activation='relu')(merged)\n",
        "\n",
        "# Output Layer\n",
        "output = Dense(1, activation='linear', name='output')(merged) # Linear activation for regression\n",
        "\n",
        "model = Model(inputs=[gene_input, fp_input], outputs=output)\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTrvAH20x2pv",
        "outputId": "a3307cd5-56cd-47c4-d4b6-4ab41359f7fe"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    [X_train_genes, X_train_fp],\n",
        "    y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jiA-k0iVSTLd",
        "outputId": "65836b18-2718-4ff0-ae7a-1bd64558c273"
      },
      "outputs": [],
      "source": [
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss = model.evaluate([X_test_genes, X_test_fp], y_test, verbose=0)\n",
        "print(f\"Test Set Mean Squared Error: {test_loss:.4f}\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict([X_test_genes, X_test_fp]).flatten()\n",
        "\n",
        "# Plot true vs. predicted values\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(y_test, y_pred, alpha=0.3)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel('True IC50 Values')\n",
        "plt.ylabel('Predicted IC50 Values')\n",
        "plt.title('True vs. Predicted IC50')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nxDym-pXTbI",
        "outputId": "55856278-f02a-476b-d183-2692d5642845"
      },
      "outputs": [],
      "source": [
        "# Model 2\n",
        "\n",
        "# Gene Expression Branch\n",
        "gene_input_v2 = Input(shape=(X_train_genes.shape[1],), name='gene_input_v2')\n",
        "gene_branch_v2 = Dense(128, activation='relu')(gene_input_v2)\n",
        "gene_branch_v2 = Dropout(0.5)(gene_branch_v2)\n",
        "gene_branch_v2 = Dense(64, activation='relu')(gene_branch_v2)\n",
        "\n",
        "# Drug Fingerprint Branch\n",
        "fp_input_v2 = Input(shape=(X_train_fp.shape[1],), name='fp_input_v2')\n",
        "fp_branch_v2 = Dense(64, activation='relu')(fp_input_v2)\n",
        "fp_branch_v2 = Dropout(0.5)(fp_branch_v2)\n",
        "fp_branch_v2 = Dense(32, activation='relu')(fp_branch_v2)\n",
        "\n",
        "# Merge Branches\n",
        "merged_v2 = Concatenate()([gene_branch_v2, fp_branch_v2])\n",
        "merged_v2 = Dense(128, activation='relu')(merged_v2)\n",
        "merged_v2 = Dropout(0.5)(merged_v2)\n",
        "\n",
        "# Output Layer\n",
        "output_v2 = Dense(1, activation='linear', name='output_v2')(merged_v2)\n",
        "\n",
        "# Create and Compile Model\n",
        "model_v2 = Model(inputs=[gene_input_v2, fp_input_v2], outputs=output_v2)\n",
        "model_v2.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Stop training if val_loss doesn't improve for 5 consecutive epochs\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history_v2 = model_v2.fit(\n",
        "    [X_train_genes, X_train_fp],\n",
        "    y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=100,\n",
        "    batch_size=128,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c394b2a"
      },
      "source": [
        "## Model 1 Results\n",
        "\n",
        "After training Model 1, we can observe the following from the loss plot and the test set evaluation:\n",
        "\n",
        "The training loss and validation loss both decrease initially, but the fluctuation in the validation loss suggests that the model is overfitting or not learning sufficiently.\n",
        "\n",
        "The Mean Squared Error on the test set is approximately 4.67.\n",
        "\n",
        "The scatter plot shows the relationship between the true log(IC50) values and the predicted values on the test set. A perfect model would follow the red dotted line, but as we see, this model is bunching its predictions amongst certain values, with quite a lot of dispersion around the true value line.\n",
        "\n",
        "This suggests there is room for improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ccec3c4"
      },
      "source": [
        "## Model 2\n",
        "\n",
        "Based on the results from Model 1, we'll make some adjustments to the NN architecture and training process to try and improve.\n",
        "\n",
        "We'll reduce the layer sizes to see if if this will give an improvement in the overfitting.\n",
        "\n",
        "**Gene Expression Branch:** Reduced from (256, 128) to (128, 64).  \n",
        "**Drug Fingerprint Branch:** Reduced from (128, 64) to (64, 32).  \n",
        "**Merged Layers:** Reduced from (256, 128) to (128).  \n",
        "\n",
        "\n",
        "We'll also increase the dropout rate from 0.3 and 0.4 to 0.5.\n",
        "\n",
        "We'll also add an early stopping callback. This monitors the validation loss during training and will stop the training process if the validation loss does not improve for a specified number of epochs (patience=5). It also restores the model weights from the epoch with the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ike7rW9o9-Ra",
        "outputId": "6aa53f82-b424-4e64-e303-478771157a91"
      },
      "outputs": [],
      "source": [
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(history_v2.history['loss'])\n",
        "plt.plot(history_v2.history['val_loss'])\n",
        "plt.title('Model v2 Loss')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss_v2 = model_v2.evaluate([X_test_genes, X_test_fp], y_test, verbose=0)\n",
        "print(f\"Test Set Mean Squared Error (v2): {test_loss_v2:.4f}\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_v2 = model_v2.predict([X_test_genes, X_test_fp]).flatten()\n",
        "\n",
        "# Plot true vs. predicted values\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(y_test, y_pred_v2, alpha=0.3)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel('True IC50 Values')\n",
        "plt.ylabel('Predicted IC50 Values')\n",
        "plt.title('True vs. Predicted IC50 (Model v2)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "411570e2"
      },
      "source": [
        "## Model 2 Results\n",
        "\n",
        "After training Model 2 with reduced complexity and increased regularization, the test set MSE is Approximately 5.6m which is higher than Model 1's MSE of 4.67. In the true vs. predicted plot, this was similar to Model 1. The predictions show dispersion around the ideal line, and there's still a tendency to cluster predictions around certain values.\n",
        "\n",
        "Despite the changes aimed at reducing overfitting, Model 2 did not outperform Model 1 on the test set based on the MSE. The early stopping helped manage the training process, but the simpler architecture and increased dropout might have limited the model's capacity to capture the underlying patterns in the data as effectively as Model 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkkotUdgqBfy"
      },
      "source": [
        "## Model 3\n",
        "\n",
        "For the third model, we'll employ [Keras Tuner](https://keras.io/keras_tuner/) to automatically sweep through a range of hyperparameters and return the best model for validation loss.\n",
        "\n",
        "The hyperparameters we are modulating include the number of neural network units, the dropout rates, and the learning rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88a5400b"
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "523633b0"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "\n",
        "def build_model(hp):\n",
        "    # Gene Expression Branch\n",
        "    gene_input = Input(shape=(X_train_genes.shape[1],), name='gene_input')\n",
        "    gene_branch = Dense(units=hp.Int('gene_dense_1_units', min_value=64, max_value=512, step=32), activation='relu')(gene_input)\n",
        "    gene_branch = Dropout(hp.Float('gene_dropout_1', min_value=0.1, max_value=0.5, step=0.1))(gene_branch)\n",
        "    gene_branch = Dense(units=hp.Int('gene_dense_2_units', min_value=32, max_value=256, step=32), activation='relu')(gene_branch)\n",
        "\n",
        "    # Drug Fingerprint Branch\n",
        "    fp_input = Input(shape=(X_train_fp.shape[1],), name='fp_input')\n",
        "    fp_branch = Dense(units=hp.Int('fp_dense_1_units', min_value=32, max_value=256, step=32), activation='relu')(fp_input)\n",
        "    fp_branch = Dropout(hp.Float('fp_dropout_1', min_value=0.1, max_value=0.5, step=0.1))(fp_branch)\n",
        "    fp_branch = Dense(units=hp.Int('fp_dense_2_units', min_value=16, max_value=128, step=16), activation='relu')(fp_branch)\n",
        "\n",
        "    # Merge Branches\n",
        "    merged = Concatenate()([gene_branch, fp_branch])\n",
        "    merged = Dense(units=hp.Int('merged_dense_1_units', min_value=64, max_value=512, step=32), activation='relu')(merged)\n",
        "    merged = Dropout(hp.Float('merged_dropout_1', min_value=0.1, max_value=0.5, step=0.1))(merged)\n",
        "    merged = Dense(units=hp.Int('merged_dense_2_units', min_value=32, max_value=256, step=32), activation='relu')(merged)\n",
        "\n",
        "\n",
        "    # Output Layer\n",
        "    output = Dense(1, activation='linear', name='output')(merged)\n",
        "\n",
        "    model = Model(inputs=[gene_input, fp_input], outputs=output)\n",
        "\n",
        "    # Tune the learning rate for the optimizer\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
        "                  loss='mean_squared_error')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bafa0bb9",
        "outputId": "f3f0a097-9989-41fb-a33b-fce19dcb7ac1"
      },
      "outputs": [],
      "source": [
        "# Instantiate the tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=10,  # Number of hyperparameter combinations to try\n",
        "    executions_per_trial=2,  # Number of models to train per combination\n",
        "    directory='keras_tuner_dir', # Directory to save results\n",
        "    project_name='drug_efficacy_tuning'\n",
        ")\n",
        "\n",
        "# Define Early Stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Run the hyperparameter search\n",
        "tuner.search(\n",
        "    [X_train_genes, X_train_fp],\n",
        "    y_train,\n",
        "    epochs=50, # Maximum number of epochs to train each model\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(\"\\nOptimal Hyperparameters found:\")\n",
        "print(f\"Gene Dense 1 Units: {best_hps.get('gene_dense_1_units')}\")\n",
        "print(f\"Gene Dropout 1: {best_hps.get('gene_dropout_1')}\")\n",
        "print(f\"Gene Dense 2 Units: {best_hps.get('gene_dense_2_units')}\")\n",
        "print(f\"FP Dense 1 Units: {best_hps.get('fp_dense_1_units')}\")\n",
        "print(f\"FP Dropout 1: {best_hps.get('fp_dropout_1')}\")\n",
        "print(f\"FP Dense 2 Units: {best_hps.get('fp_dense_2_units')}\")\n",
        "print(f\"Merged Dense 1 Units: {best_hps.get('merged_dense_1_units')}\")\n",
        "print(f\"Merged Dropout 1: {best_hps.get('merged_dropout_1')}\")\n",
        "print(f\"Merged Dense 2 Units: {best_hps.get('merged_dense_2_units')}\")\n",
        "print(f\"Learning Rate: {best_hps.get('learning_rate')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "c0790129",
        "outputId": "c8248063-932c-4400-9309-6525261a8edd"
      },
      "outputs": [],
      "source": [
        "# Get the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "test_loss_tuned = best_model.evaluate([X_test_genes, X_test_fp], y_test, verbose=0)\n",
        "print(f\"\\nTest Set Mean Squared Error (Tuned Model): {test_loss_tuned:.4f}\")\n",
        "\n",
        "# Make predictions on the test set with the best model\n",
        "y_pred_tuned = best_model.predict([X_test_genes, X_test_fp]).flatten()\n",
        "\n",
        "# Plot true vs. predicted values for the tuned model\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(y_test, y_pred_tuned, alpha=0.3)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel('True IC50 Values')\n",
        "plt.ylabel('Predicted IC50 Values')\n",
        "plt.title('True vs. Predicted IC50 (Tuned Model)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1543e9c"
      },
      "source": [
        "## Model 3 Results\n",
        "\n",
        "The hyperparameter tuning didn't produce markedly different results than the earlier models, and was even slightly worse than the first model. The true-vs-predicted plot shows no improvement with the predicted values still bunching in single-valued bands. This suggests that we need to further investigate other architectural changes like batch normalization or other normalization techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abcdcfcc"
      },
      "source": [
        "## Model 4\n",
        "\n",
        "As a last attempt, we'll define a model based on the Model 1 choice of hyperparameters, but with the addition of batch normalization layers after each dense layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "id": "a2342eb7",
        "outputId": "42fbb855-79d3-46d7-ef3a-af54abe9a796"
      },
      "outputs": [],
      "source": [
        "# Model 4 with Batch Normalization\n",
        "\n",
        "# Gene Expression Branch\n",
        "gene_input_bn = Input(shape=(X_train_genes.shape[1],), name='gene_input_bn')\n",
        "gene_branch_bn = Dense(256, activation='relu')(gene_input_bn)\n",
        "gene_branch_bn = tf.keras.layers.BatchNormalization()(gene_branch_bn)\n",
        "gene_branch_bn = Dropout(0.3)(gene_branch_bn)\n",
        "gene_branch_bn = Dense(128, activation='relu')(gene_branch_bn)\n",
        "gene_branch_bn = tf.keras.layers.BatchNormalization()(gene_branch_bn)\n",
        "\n",
        "\n",
        "# Drug Fingerprint Branch\n",
        "fp_input_bn = Input(shape=(X_train_fp.shape[1],), name='fp_input_bn')\n",
        "fp_branch_bn = Dense(128, activation='relu')(fp_input_bn)\n",
        "fp_branch_bn = tf.keras.layers.BatchNormalization()(fp_branch_bn)\n",
        "fp_branch_bn = Dropout(0.3)(fp_branch_bn)\n",
        "fp_branch_bn = Dense(64, activation='relu')(fp_branch_bn)\n",
        "fp_branch_bn = tf.keras.layers.BatchNormalization()(fp_branch_bn)\n",
        "\n",
        "\n",
        "# Merge Branches\n",
        "merged_bn = Concatenate()([gene_branch_bn, fp_branch_bn])\n",
        "merged_bn = Dense(256, activation='relu')(merged_bn)\n",
        "merged_bn = tf.keras.layers.BatchNormalization()(merged_bn)\n",
        "merged_bn = Dropout(0.4)(merged_bn)\n",
        "merged_bn = Dense(128, activation='relu')(merged_bn)\n",
        "merged_bn = tf.keras.layers.BatchNormalization()(merged_bn)\n",
        "\n",
        "\n",
        "# Output Layer\n",
        "output_bn = Dense(1, activation='linear', name='output_bn')(merged_bn)\n",
        "\n",
        "# Create and Compile Model\n",
        "model_bn = Model(inputs=[gene_input_bn, fp_input_bn], outputs=output_bn)\n",
        "model_bn.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "model_bn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "660d78ff",
        "outputId": "b64983b6-48f6-4aee-d69a-f067c840deb4"
      },
      "outputs": [],
      "source": [
        "# Define Early Stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model with Batch Normalization\n",
        "history_bn = model_bn.fit(\n",
        "    [X_train_genes, X_train_fp],\n",
        "    y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b0fbd704",
        "outputId": "d073763c-04a1-43cf-da27-d25dfd5b6aed"
      },
      "outputs": [],
      "source": [
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(history_bn.history['loss'])\n",
        "plt.plot(history_bn.history['val_loss'])\n",
        "plt.title('Model with Batch Normalization Loss')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss_bn = model_bn.evaluate([X_test_genes, X_test_fp], y_test, verbose=0)\n",
        "print(f\"Test Set Mean Squared Error (Model with Batch Normalization): {test_loss_bn:.4f}\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_bn = model_bn.predict([X_test_genes, X_test_fp]).flatten()\n",
        "\n",
        "# Plot true vs. predicted values\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(y_test, y_pred_bn, alpha=0.3)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel('True IC50 Values')\n",
        "plt.ylabel('Predicted IC50 Values')\n",
        "plt.title('True vs. Predicted IC50 (Model with Batch Normalization)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2f98d62d",
        "outputId": "1e7716e8-3318-4e71-d395-b9c86a599e41"
      },
      "outputs": [],
      "source": [
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(history_bn.history['loss'])\n",
        "plt.plot(history_bn.history['val_loss'])\n",
        "plt.title('Model with Batch Normalization Loss')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss_bn = model_bn.evaluate([X_test_genes, X_test_fp], y_test, verbose=0)\n",
        "print(f\"Test Set Mean Squared Error (Model with Batch Normalization): {test_loss_bn:.4f}\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_bn = model_bn.predict([X_test_genes, X_test_fp]).flatten()\n",
        "\n",
        "# Plot true vs. predicted values\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(y_test, y_pred_bn, alpha=0.3)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel('True IC50 Values')\n",
        "plt.ylabel('Predicted IC50 Values')\n",
        "plt.title('True vs. Predicted IC50 (Model with Batch Normalization)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3180d58"
      },
      "source": [
        "## Model 4 Results\n",
        "\n",
        "This model with Batch Normalization achieved an MSE of ~4.2848 on the test set, which was the best so far. The scatter plot of true versus predicted IC50 values shows a general positive correlation but still shows significant dispersion around the ideal prediction line. At least the batch normalization seems to be correcting the earlier models' tendency to output only select IC 50 values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4411d1e9"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This project aimed to build a deep learning model to predict drug efficacy based on cancer cell gene expression and drug molecular structure. I explored several model architectures, including a multi-input neural network, a tuned version using KerasTuner, and a model incorporating batch normalization.\n",
        "\n",
        "While the models showed some ability to predict log(IC50) values, as indicated by the positive correlation in the true vs. predicted plots, the MSE values suggest there is significant room for improvement. The clustering of predictions around certain values was a persistent issue across models except the last model, where batch normalization seems to provide some improvement. More tuning and experimentation would be necessary to discover further improvements.\n",
        "\n",
        "Despite not achieving high predictive accuracy from these models, I learned a great deal about working with bioinformatics data, and I now have a glimplse into building and evaluating deep learning models for regression tasks.\n",
        "\n",
        "I'm still deeply interested in this topic and plan to continue exploring it in the future. Potential future steps include investigating different model architectures (graph neural networks, etc.), trying other machine learning algorithms suited tabular data, and potentially incorporating additional biological and chemical features from the GDSC dataset."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
